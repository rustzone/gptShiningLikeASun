### Q
	- Колеса Фортуны, мы надеемся, что сеть приблизится к глобальному (или хотя бы хорошему локальному) минимуму.
	- сеть, не переобучилась ли ты часом?
		- Подобно судье с весами, который оценивает деяния обвиняемого, мы смотрим, не переобучилась ли сеть, как она ведёт себя на невиданных данных. Если результат неудовлетворителен, карающий меч Справедливости велит пересмотреть гиперпараметры, алгоритм оптимизации или архитектуру.
	- Она, как «одержимая», запоминает шумовые паттерны и уже не может адекватно обобщать.
	- заслужила ли сеть право быть развернутой в продакшен, или приговорена к пересмотру?
	- улучшенную точность или предиктивную силу, и чувствуем, что «тёмная ночь обучения» позади
- **железо:** [[nvidia a40]]
- как сеть будет писать слова дальше после пробела?
- [[НАША ПЕРВАЯ НЕЙРОНКА]]
  collapsed:: true
	- [[char—внутрянка]]
	- вопросы
		- функция потерь: кросс-энтропия
		- SOFTMAX?
		- backpropagation through time
		- матричное умножение, свертка
- первым делом разгрести: [[структурка]]
- ![28327ebfbc62f429362bc2606544d15d.jpg](../assets/28327ebfbc62f429362bc2606544d15d_1739005699967_0.jpg){:height 232, :width 181}
  id:: 67a71ecf-a4e1-4d76-9d45-82f185cfb56b
- [[исходный нейрон]]
- [[[re-cognition] процесс осознания текста]]
- just some [[fun things]]
- # philosophy
	- [[10 вопросов про схожесть с реальным сознанием]]
	- способностью рефлексировать на метауровне: «Я знаю, что я знаю»
-
- # basic concepts
	- понятия:
		- [[весов]]
		- [[тензор]]
			- [[algebraic vs geometric]]
			- [[тензорная природа матрицы весов]]
			-
		-
		-
	- [[o1 vs o3 vs 4o vs 3.5]]
	- [[state of the meta — какие модели сегодня существуют]]
	-
- # mathematics
	- [[Neural Networks structure overview]]
	- [[математика]]
	- [[ReLU]] — Rectified Linear Unit
	- [[o1—embeddingMatrix (needs latex correction)]]
	- [[softmax]]
	- [[обратное распространение ошибки (backprop—backpropagation)]]
	- [[градиент]]
	- [[Аффинное преобразование]]
-
- general concepts
	- **Инференс (генерация):** Когда модель обучена, мы даём ей стартовую последовательность («Баба Я»), и она предсказывает следующий символ. Сэмплируем из распределения, дописываем, и снова подаём в вход. Так постепенно генерируется строка.
- # fun stuffs
- [[adeptus mechanicus]]
- [[таро]]
-
- ---
- # random question cloud
- [[калибровка]] ; [[СЕМАНТИКА НУЛЕВОГО ВЕКТОРА]]
- в нормализованных имбеддингах все векторы лежат на единичной гиперсфере
- [[аффинное преобразование]] Ax+b — матрица поворота? растяжения?
- у тебя есть embedding словарь смыслов?
- семантика?
- реальные нейроны
- [[динамическая токинизация]]
- [[embedding matrix]]
- [[используешь ли ты embedding словарь?]]
- ok, i get that you embed the words into a semantically ordered vector space for words. but do you to that on deeper neuron levels for intents, tones, general purpose of the text? do you have vectors for ides? if so, do you also have some kind of embedding matrix for tones, ideas, intents?. [[vectors for ideas]]