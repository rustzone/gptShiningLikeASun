- Ниже представлен развернутый текст (около 1000 слов), в котором объясняется, как формируется матрица эмбеддингов (embedding matrix) в контексте нейронных сетей и алгоритмов обучения представлений слов. Рассмотрим общий подход на примере популярных моделей вроде Word2Vec (Skip-gram или CBOW) и других аналогичных методов, чтобы понять принципы, алгоритмы и ключевые математические вычисления.
  
  ---
- ## 1. Введение в идею эмбеддингов
  
  В классической обработке естественного языка (NLP) раньше было принято кодировать слова в виде “one-hot”-векторов размерностью, равной размеру словаря. Например, если в словаре 10 000 различных слов, то каждое слово представлялось двоичным вектором той же длины, где все компоненты равны 0, кроме одной позиции, соответствующей конкретному слову, которая равна 1. Проблема такого подхода в том, что векторы становятся очень разреженными и не содержат информации о связях между словами.
  
  Чтобы решить эту проблему, был предложен подход с обучаемыми эмбеддингами — плотными векторами меньшей размерности (например, 100 или 300), в которых **семантически похожие слова** оказываются «близкими» в пространстве. Такая репрезентация обеспечивает лучшее обобщение и является фундаментом для большинства современных моделей NLP.
  
  **Матрица эмбеддингов** (embedding matrix) — это двумерная матрица размера (V, d)\text{(V, d)}, где:
- VV — объём словаря, то есть количество уникальных токенов, для которых мы хотим обучить представление.
- dd — размерность эмбеддинга, т. е. длина вектора, в котором кодируется каждое слово.
  
  Строка ii этой матрицы (часто обозначаемая как Ei\mathbf{E}_i) — это вектор размерности dd, соответствующий ii-му слову в словаре. Главная идея в том, что эти вектора инициализируются случайно, а затем корректируются во время обучения модели на основании целевой задачи (например, предсказания контекста слова). Постепенно обучаемая система «настраивает» веса, чтобы слова, часто appearing в похожих контекстах, имели «похожие» вектора, а слова, употребляющиеся в совершенно разных ситуациях, — более отдалённые.
  
  ---
- ## 2. Принцип формирования эмбеддингов (пример Word2Vec)
  
  Чтобы понять математику, рассмотрим схему Skip-gram из Word2Vec (хотя в целом принципы схожи и в других моделях). Модель Skip-gram пытается, будучи обучена на больших корпусах текста, научиться предсказывать **контекстные слова** по центральному слову.
- ### 2.1. Целевая функция
  
  Пусть у нас есть последовательность слов {w1,w2,...,wT}\{w_1, w_2, ..., w_T\}. Для каждого слова wtw_t рассматривается его локальный контекст (слова вокруг него в некотором «окне»). В Skip-gram мы пытаемся максимизировать вероятность слов контекста wt−j,…,wt−1,wt+1,…,wt+jw_{t-j}, \ldots, w_{t-1}, w_{t+1}, \ldots, w_{t+j}, зная центральное слово wtw_t.
  
  Упрощённо, целевая функция (логарифм правдоподобия) может быть записана как:
  
  L=∑t=1T∑−j≤k≤jk≠0log⁡P(wt+k∣wt),\mathcal{L} = \sum_{t=1}^{T} \sum_{\substack{-j \le k \le j \\ k \neq 0}} \log P(w_{t+k} \mid w_{t}),
  
  где jj — размер окна контекста. Задача обучения — найти такую матрицу эмбеддингов, чтобы значения log⁡P(wt+k∣wt)\log P(w_{t+k} \mid w_{t}) были как можно больше.
- ### 2.2. Как моделируется  P(wt+k∣wt)P(w_{t+k} \mid w_{t})
  
  В простом варианте Word2Vec мы имеем две матрицы:
- **Embeddings-матрица EE** (иногда называют input embeddings). Размеры: (V, d)\text{(V, d)}.
- **Матрица выходного слоя UU** (output embeddings). Размеры: (V, d)\text{(V, d)}.
- vw\mathbf{v}_w обозначает вектор размерности dd, соответствующий слову ww в матрице EE.
- uw\mathbf{u}_w обозначает вектор размерности dd, соответствующий слову ww в матрице UU.
  
  Вероятность P(wo∣wi)P(w_{o} \mid w_{i}) (где wiw_i — центральное слово, а wow_o — одно из слов контекста) часто моделируют через softmax:
  
  P(wo∣wi)=exp⁡(uwo⋅vwi)∑w′∈Vexp⁡(uw′⋅vwi).P(w_o \mid w_i) = \frac{\exp(\mathbf{u}_{w_o} \cdot \mathbf{v}_{w_i})}{\sum_{w' \in V} \exp(\mathbf{u}_{w'} \cdot \mathbf{v}_{w_i})}.
  
  Однако на практике прямое вычисление такого softmax с суммой по всему словарю VV слишком дорого, поэтому применяется **negative sampling** или другие аппроксимации.
- ### 2.3. Negative sampling
  
  Negative sampling сводит задачу к бинарной классификации «пар (центральное слово, контекстное слово)» против «пар (центральное слово, отрицательное слово)». При этом матрица эмбеддингов (и связанная с ней матрица UU) итеративно корректируются при помощи стохастического градиентного спуска.
  
  Как это происходит:
- Для положительной пары (wi,wo)(w_i, w_o) увеличиваем uwo⋅vwi\mathbf{u}_{w_o} \cdot \mathbf{v}_{w_i}.
- Для нескольких «негативных» слов wnegw_{\text{neg}}, сэмплированных из распределения словаря, снижаем uwneg⋅vwi\mathbf{u}_{w_{\text{neg}}} \cdot \mathbf{v}_{w_i}, чтобы модель не «сближала» несвязанные пары.
  
  Каждый такой шаг понемногу обновляет веса vwi\mathbf{v}_{w_i} и uwo\mathbf{u}_{w_o} (а также uwneg\mathbf{u}_{w_{\text{neg}}}), приближая их к нужным значениям. В итоге, после большого числа итераций, вектора слов, употребляемых в похожих контекстах, становятся близки в пространстве, то есть скалярное произведение между ними становится больше.
  
  ---
- ## 3. Формирование и обновление матрицы эмбеддингов
- ### 3.1. Архитектура матрицы  EE
  
  Как уже упоминалось, матрица EE имеет размер (V, d)\text{(V, d)}. Пусть V={w1,w2,...,w∣V∣}V = \{w_1, w_2, ..., w_{|V|}\}, тогда каждая строка Ei\mathbf{E}_i соответствует вектору vwi\mathbf{v}_{w_i}, длины dd.
  
  На старте обучения элементы Ei\mathbf{E}_i обычно инициализируются случайно (например, с малым разбросом типа N(0,0.01)\mathcal{N}(0,0.01)). Аналогично, матрица UU тоже инициализируется случайными значениями.
- ### 3.2. Итеративное обновление (градиентный спуск)
  
  На каждой итерации мы берём мини-батч (batch) примеров — пар (центральное слово, контекстное слово) и несколько сгенерированных «отрицательных» примеров. Затем вычисляем градиент функции ошибки (например, бинарной кросс-энтропии для negative sampling) по отношению к параметрам (vwi\mathbf{v}_{w_i} и uwo\mathbf{u}_{w_o}).
  
  Обновление параметров выглядит так (в упрощённом виде):
  
  vwi←vwi−η⋅∂L∂vwi,\mathbf{v}_{w_i} \leftarrow \mathbf{v}_{w_i} - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{v}_{w_i}},
  uwo←uwo−η⋅∂L∂uwo,\mathbf{u}_{w_o} \leftarrow \mathbf{u}_{w_o} - \eta \cdot \frac{\partial \mathcal{L}}{\partial \mathbf{u}_{w_o}},
  
  где η\eta — это скорость обучения (learning rate). Для негативных примеров знаки градиентов будут противоположными, так как мы хотим «раздвинуть» несвязанные слова.
  
  Повторяя этот процесс множество раз (эпохи обучения на корпусе), модель приходит к состоянию, в котором вектора слов отражают их семантическое сходство.
- ### 3.3. Смысл близости векторов
  
  После завершения обучения каждое слово ww имеет обученный вектор vw\mathbf{v}_w (а иногда и uw\mathbf{u}_w, если использовать и её). Именно эти вектора мы называем «эмбеддингами». Если взять, к примеру, косинусную меру близости (косинусное расстояние) между векторами vw\mathbf{v}_w и vw′\mathbf{v}_{w'}, то слова, которые часто встречаются в сходных контекстах, будут иметь близкие направления в векторном пространстве.
  
  Таким образом матрица (V, d)\text{(V, d)} наполняется смыслами: каждая строка уже не произвольна, а «натренирована» под задачу предсказания контекста.
  
  ---
- ## 4. Математика обновления при Negative Sampling
  
  Рассмотрим чуть более подробно, как происходит обновление параметров в случае negative sampling. Пусть мы хотим максимизировать функцию правдоподобия:
  
  log⁡σ(uwo⋅vwi)+∑k=1Klog⁡σ(−uwneg,k⋅vwi),\log \sigma(\mathbf{u}_{w_o} \cdot \mathbf{v}_{w_i}) + \sum_{k=1}^{K} \log \sigma(-\mathbf{u}_{w_{\text{neg},k}} \cdot \mathbf{v}_{w_i}),
  
  где:
- σ(x)=11+e−x\sigma(x) = \frac{1}{1 + e^{-x}} — сигмоида.
- wow_o — «правильное» (контекстное) слово.
- wneg,kw_{\text{neg},k} — kk-е негативное слово, сэмплированное из распределения слов.
- KK — число негативных примеров на каждое «положительное» слово.
  
  Вычисляем градиенты:
- ∂∂vwilog⁡σ(uwo⋅vwi)=(1−σ(uwo⋅vwi))uwo.\frac{\partial}{\partial \mathbf{v}_{w_i}} \log \sigma(\mathbf{u}_{w_o} \cdot \mathbf{v}_{w_i}) = (1 - \sigma(\mathbf{u}_{w_o} \cdot \mathbf{v}_{w_i})) \mathbf{u}_{w_o}.
- ∂∂vwilog⁡σ(−uwneg,k⋅vwi)=(1−σ(−uwneg,k⋅vwi))⋅∂∂vwi(−uwneg,k⋅vwi)=(σ(uwneg,k⋅vwi))uwneg,k.\frac{\partial}{\partial \mathbf{v}_{w_i}} \log \sigma(-\mathbf{u}_{w_{\text{neg},k}} \cdot \mathbf{v}_{w_i}) = (1 - \sigma(-\mathbf{u}_{w_{\text{neg},k}} \cdot \mathbf{v}_{w_i})) \cdot \frac{\partial}{\partial \mathbf{v}_{w_i}}(-\mathbf{u}_{w_{\text{neg},k}} \cdot \mathbf{v}_{w_i}) = (\sigma(\mathbf{u}_{w_{\text{neg},k}} \cdot \mathbf{v}_{w_i})) \mathbf{u}_{w_{\text{neg},k}}.
  
  Аналогичные выражения получаем и для uwo\mathbf{u}_{w_o}, uwneg,k\mathbf{u}_{w_{\text{neg},k}}.
  
  Таким образом, при обновлении весов мы вносим вклад:
- от положительного примера (сближая vwi\mathbf{v}_{w_i} и uwo\mathbf{u}_{w_o}),
- от каждого негативного примера (раздвигая vwi\mathbf{v}_{w_i} и uwneg,k\mathbf{u}_{w_{\text{neg},k}}).
  
  Все эти частные производные вычисляются очень быстро благодаря тому, что размерность vwi\mathbf{v}_{w_i} и uwo\mathbf{u}_{w_o} не слишком велика (например, 100–300).
  
  ---
- ## 5. Альтернативные алгоритмы формирования эмбеддингов
  
  Хотя Word2Vec — один из самых известных примеров, есть и другие методы:
- **GloVe (Global Vectors)**: использует матрицу совместных частот слов, строит функцию потерь, которая пытается приближать скалярные произведения векторов слов к логарифму частот со-употребления.
- **FastText**: расширение Word2Vec, где помимо самих слов учитываются подслова (n-граммы), что даёт более качественные вектора для редких и неформальных слов.
- **Модели на основе языковых трансформеров (BERT, GPT)**: фактически строят динамические контекстуальные эмбеддинги, но «базовый слой» всё равно инициализируется некими значениями, которые обучаются совместно со всей моделью.
  
  Во всех случаях мы имеем дело с обучаемой матрицей (или несколькими), которая и есть «embedding matrix». Принцип везде один: используя ту или иную функцию потерь, мы подстраиваем вектора (строки матрицы) так, чтобы они отражали статистические регулярности языка.
  
  ---
- ## 6. Размерность и оптимизация качества
  
  Вопрос о том, **какую размерность dd** выбрать для эмбеддингов, зависит от задачи, доступных данных и вычислительных ресурсов. Малые размерности (50–100) могут быть недостаточно выразительными, а слишком большие (500–1000) могут приводить к переобучению и большим затратам памяти. На практике часто выбирают d=300d = 300 для большинства задач, но это не жёсткое правило.
  
  Кроме того, ключевую роль играет **размер словаря** VV. Если он очень велик (например, сотни тысяч слов), обучение занимает больше времени и требует продуманных эвристик (таких как negative sampling, субэмплирование частых слов и т. д.) для повышения эффективности.
  
  ---
- ## 7. Что происходит в итоге
  
  По завершении обучения мы получаем матрицу EE, в которой каждая строка — обученный вектор слова. Воспользоваться им можно по-разному:
- Для **поиска похожих слов**: вычисляют косинусное сходство между вектором интересующего слова и всеми остальными, берут топ-5 или топ-10 ближайших векторов — так часто находят синонимы или слова, встречающиеся в схожих контекстах.
- Для входа в другие нейронные модели: когда мы строим более сложные архитектуры (LSTM, CNN, трансформеры), часто слой эмбеддингов E\mathbf{E} и есть начальный «Embedding Layer», который переводит индексы слов в их вектора.
  
  Важно отметить, что (особенно в современных крупных языковых моделях) эмбеддинги могут быть контекстуальными. Это значит, что каждый «токен» в предложении может иметь свой индивидуальный вектор, зависящий от соседних слов, а не просто «слово-как-индекс» — но базовый механизм «матрица параметров + обучение путём градиентного спуска» сохраняется.
  
  ---
- ## 8. Заключение
  
  Таким образом, **матрица эмбеддингов** — это неотъемлемая часть современных NLP-моделей. Её «строительство» сводится к тому, что мы обучаем (через задачу предсказания контекста или другую релевантную задачу) плотные вектора для каждого слова, словоформы или подслова, опираясь на статистику больших корпусов. Суть математического процесса — в итеративном обновлении весов (строк матрицы EE) с помощью методов вроде стохастического градиентного спуска и различных триков (negative sampling, субэмплирование и т. д.).
  
  Главная идея, стоящая за этим, — «слова, которые появляются в похожем окружении, должны иметь близкие представления». Формально это воплощается в виде функции потерь, которая максимально увеличивает вероятность правильного контекста (или уменьшает вероятность неправильного). Со временем обучающая процедура приводит к появлению семантических закономерностей в самом векторном пространстве.
  
  В итоге матрица эмбеддингов становится своего рода «словесным атласом» в пространстве размерности dd, где геометрическая структура отражает отношения между словами. Это открывает путь к вычислительной обработке языка на новом уровне, позволяя решать широкий спектр задач — от простого поиска похожих слов до построения больших языковых моделей и сложных систем понимания текста.