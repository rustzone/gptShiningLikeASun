## What Does an Embedding Matrix Look Like?
	- An embedding matrix is essentially a large numerical table (more formally, a 2D tensor) where:
		- Each **row** corresponds to a token from the model’s vocabulary.
		- Each **column** corresponds to one dimension in the token embedding vector.
	- If we label the size of the vocabulary by VV and the dimensionality of the embedding by dd, the shape of this matrix is (V×d)(V \times d). For instance, if a model has a vocabulary of 50,000 tokens, and each token is represented as a 768-dimensional vector, then the embedding matrix’s shape is (50000×768)(50000 \times 768).
- ### A Small, Illustrative Example
	- Obviously, real-world embedding matrices are huge. But here’s a tiny, contrived example. Let’s say our entire vocabulary (VV) has only 5 tokens. We’ll call them:
	    1. Token index 0 → `"[PAD]"` (padding or some special token)
	    2. Token index 1 → `"Hello"`
	    3. Token index 2 → `"world"`
	    4. Token index 3 → `"!"`
	    5. Token index 4 → `"[UNK]"` (unknown token)
	- And we have an embedding dimension d=4d = 4. That means each token is mapped to a 4-dimensional vector. The embedding matrix EE will look something like this (numbers are made-up, but they give the idea):
	- ```plain text
	   E = [
	     [  0.12,  -0.05,   0.66,   0.10 ],   # Embedding for token index 0: "[PAD]"
	     [  0.90,   0.01,  -0.21,   0.35 ],   # Embedding for token index 1: "Hello"
	     [ -0.11,   0.55,   0.00,  -0.67 ],   # Embedding for token index 2: "world"
	     [  0.02,   0.15,   1.01,   0.04 ],   # Embedding for token index 3: "!"
	     [ -0.75,   0.33,  -0.14,   0.59 ]    # Embedding for token index 4: "[UNK]"
	   ]
	  ```
	- In a more matrix-like representation:
	- {{[[table]]}}
		- Token Index
			- Dim 0
				- Dim 1
					- Dim 2
						- Dim 3
		- **0** (`"[PAD]"`)
			- 0.12
				- -0.05
					- 0.66
						- 0.10
		- **1** (`"Hello"`)
			- 0.90
				- 0.01
					- -0.21
						- 0.35
		- **2** (`"world"`)
			- -0.11
				- 0.55
					- 0.00
						- -0.67
		- **3** (`"!"`)
			- 0.02
				- 0.15
					- 1.01
						- 0.04
		- **4** (`"[UNK]"`)
			- -0.75
				- 0.33
					- -0.14
						- 0.59
	- When the model needs the embedding for token #2 (“world”), it goes to the second row (index 2 in zero-based indexing) and retrieves `[-0.11, 0.55, 0.00, -0.67]`.
- ## What Is “Inference Time”?
	- **Inference time** (sometimes called the __deployment__ or __serving__ phase) is when you use a trained model to generate outputs or predictions for new data/input, without changing the model’s parameters. In the context of language models like GPT, __inference time__ is when you prompt the model with a question or a phrase, and the model responds with text completions.
- ### Training vs. Inference
    1. **Training Time**
	- The model sees massive amounts of text data.
	- It continually adjusts (learns) its internal weights, including the embedding matrix, via backpropagation.
	- This process can take days or even weeks on powerful hardware (e.g., large GPU clusters).
	- After sufficient training, the model’s weights (including the embedding matrix) are “frozen” or “fixed” for general use.
	  2. **Inference Time**
	- The trained model is presented with **new** (unseen) user inputs or test data.
	- The model runs a forward pass through its network to produce an output.
	- **No** weight updates happen (i.e., no backpropagation).
	- You can think of it like “execution mode” for generating or predicting results.
	  3. In simpler words:
	- **During training**: The model __learns__ the embedding vectors (and all other weights).
	- **During inference**: The model __uses__ those learned embeddings to understand and generate text.
- ### Example: Inference Time in Action
    1. You type:
       ```plain text
       "How do I bake a chocolate cake?"
       ```
    2. The model is already trained, so it:
        1. Tokenizes your input into subword tokens (like `"How", " do", " I", " bake", " a", " chocolate", " cake", "?"`).
        2. Converts each token to an integer index (for example `4831, 195, 31, 694, 14, 8901, 645, 112` — just an example).
        3. Looks up the embedding vector for each index from the **embedding matrix**.
        4. Passes those vectors into the Transformer layers, which process them.
        5. Produces a distribution over possible next tokens at each step and finally generates a response, e.g.,
           ```plain text
           "Preheat your oven to 350°F, mix the dry ingredients..."
           ```
    3. Throughout this entire sequence (the “inference”), the model is __not__ updating or training its embedding matrix. It’s simply __accessing__ those learned embeddings and using them to understand your query.
- ### Summary
	- **Yes, an embedding matrix is like a “dictionary”** mapping each token index to a dense vector of numbers.
	- **It’s learned during training**, then used “as is” during inference, without any further modifications to the weights.
	- **Inference time** is the phase where the model __serves__ predictions (or text completions) for new inputs, relying on the embeddings and other learned parameters from the training phase.