- ### Embedding + Position Encoding
    - Каждый токен tt переходит в вектор etet​ размерности dd.
    - Позиционное кодирование добавляется (или складывается) с этим вектором, чтобы модель знала: первый это токен, пятый или сотый.
        - Результат: $$Input=e_t+p_i$$​, где $$p_i$$​ — вектор позиции $$i$$.
- ### Self-Attention
    - Здесь ключевая идея — не «последовательно» обрабатывать токен за токеном, а дать модели общий контекст.
        - Для этого вводятся три матрицы весов: $$( W_Q, W_K, W_V )$$ (Query, Key, Value).
            - **Query** $$( Q = X \cdot W_Q )$$
            - **Key** $$( K = X \cdot W_K )$$
            - **Value** $$( V = X \cdot W_V )$$
            - где $$( X )$$ — это матрица всех входных векторов (по сути, набор эмбеддингов текущего слоя).\
                - **Краткое объяснение матриц Q, K и V в механизме внимания (Self-Attention):**
                    - **Q (Query)** — задаёт «запрос» от текущего токена: какие признаки он «ищет» в других токенах.
                    - **K (Key)** — «ключ» каждого токена: какие признаки он «предлагает» или чем он «характеризуется».
                    - **V (Value)** — «значение»: векторная информация токена, которая будет передаваться и смешиваться в итоговом представлении согласно весам внимания.
                - Вычисление внимания идёт так: каждая **Query** сравнивается со всеми **Key**, чтобы найти «важные» связи. Затем итоговые веса внимания применяются к соответствующим **Value**, формируя результат self-attention.
            - Затем вычисляется «внимание» (attention scores) как ( Q K^T ). Если размерность каждого вектора ( Q_i ) и ( K_j ) — это ( d_k ), то при перемножении получаем матрицу размером ( n \times n ), где ( n ) — число токенов. Элементы этой матрицы определяют, насколько токен ( i ) «внимателен» к токену ( j ).
            - Эти «оценки внимания» масштабируются (делятся на ( \sqrt{d_k} )), проходят через софтмакс, и получаем «веса внимания».
            - «Веса внимания» затем умножаются на матрицу ( V ), и результат становится главным входом в следующий подблок.
    - Чтобы чуть глубже понять устройство сети, стоит разобрать основные механизмы архитектуры более технически:
        1. **Многоголовое внимание (Multi-Head Attention)**
            - Чтобы модель могла «смотреть» на разные аспекты информации параллельно, существует несколько голов внимания (Heads), каждая со своими матрицами ( W_Q, W_K, W_V ).
            - Итоговые результаты всех голов склеиваются (конкатенируются) и преобразуются ещё одной линейной матрицей.
        2. **Feed-Forward Network (FFN)**
            - После блока внимания идёт обычная полносвязная сеть (часто 2-слойная) с нелинейностью (ReLU или GELU).
            - Формула примерно такая:
            - [
                \text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2
              ]
            - То есть каждый токен-вектор обрабатывается по отдельности, но с большим количеством внутренних параметров.
        3. **Соединение слоёв**
            - Каждый слой Трансформера повторяет: (1) Многоголовое внимание + (2) FFN.
            - Между этими двумя шагами и после них, как правило, делают механизм «residual connections» (перекидывание исходного входа вперёд) + нормализацию (Layer Normalization).
            - Таким образом результат слоя (выход) снова становится входом для следующего слоя.
        4. **Выходной слой**
            - После N таких блоков Трансформера получается матрица размером ( n \times d ), где ( n ) — число токенов, а ( d ) — размерность скрытых представлений.
            - Далее идёт выходная (линейная) проекция на размер словаря, чтобы получить вероятность для каждого следующего токена:
            - [
                \text{Logits} = X_{\text{final}} W_{\text{out}} + b_{\text{out}}
              ]
            - Применяется софтмакс, и получаем вероятности.
- 
- ### Итого, какие «особенности»?
    - **Отсутствие «центрального» нейрона**: Сеть устроена слоями, в каждом слое — множество параметров. Нет единственной точки, из которой «ветвятся» все сигналы.
    - **Многоголовое внимание**: Это позволяет «одновременно» анализировать разные паттерны и взаимосвязи токенов.
    - **Резидуальные (skip) соединения**: Они помогают сохранить градиенты при обучении и стабилизировать его, позволяя сигналам «обходить» сложные вычислительные пути.
    - **Параллелизм**: В трансформере многие вычисления можно выполнять параллельно по всем токенам сразу, в отличие от рекуррентных сетей прошлого поколения.
    - **Возможность очень глубоких сетей**: Трансформеры обычно состоят из 12, 24 и более слоёв (блоков), и каждый блок содержит сотни миллионов параметров (в больших моделях — миллиарды).
    - ---
- ## Короткий итог
    - У ChatGPT нет единственного «исходного нейрона». Модель опирается на многослойную структуру, в каждом слое — очень много параметров (нейронов), и сигнал проходит через каскад таких блоков.
    - Нейронная сеть заканчивается тем, что выдаёт вероятностное распределение по словарю возможных следующих токенов.
    - Из одного «нейрона» (или, точнее, компоненты вектора) может «выходить» множество связей, поскольку внутри каждого слоя мы имеем матрицы весов, полностью соединяющие вход и выход.
    - Основные особенности сети — это механизм внимания (self-attention) и многоголовость (multi-head attention), позволяющие эффективно обрабатывать большой контекст текста.
