- Ниже вы найдёте большой развернутый текст (примерно на 3000 слов) про самые разные архитектуры нейронных сетей, представленные в современном мире. Я постарался сделать повествование не только информативным, но и добавить немного историй, шуток и интересных фактов, чтобы чтение не превратилось в одно сплошное перечисление аббревиатур. Приятного погружения в мир нейросетей!
  
  ---
- ## Введение: коротко о нейронных сетях и их обаянии
  
  Искусственные нейронные сети (ИНС) давно уже не новость в мире науки и технологий: первые концепции появились ещё в середине XX века, когда учёные пытались имитировать работу биологических нейронов. Однако настоящий «бум» произошёл в последнее десятилетие, когда вычислительные мощности, огромные датасеты и новые алгоритмические идеи сошлись вместе, словно планеты под счастливой звездой. В итоге мы получили то, что сегодня называется «возрождением нейронных сетей», и именно благодаря этому рывку они стали решать массу прикладных задач: от распознавания изображений и перевода текстов до сочинения музыки и даже генерации новых молекул.
  
  Быть может, вы задаётесь вопросом: чем вообще современные архитектуры отличаются друг от друга? Ведь формально каждая нейросеть — это набор параметров (весов и смещений), которые обучаются по входным данным с помощью той или иной функции потерь и градиентного спуска. Но в реальности архитектонические различия огромны: количество слоёв, типы связей, использование механизма внимания (attention) или рекуррентных слоёв, подходы к нормализации и инициализации — всё это формирует множество «семейств» моделей.
  
  В этом эссе я хотел бы провести своеобразную «экскурсию» по основным видам нейронных сетей, от классических MLP (multilayer perceptron) до суперпопулярных сегодня «трансформеров», а также коротко упомянуть о важнейших наследниках этих архитектур: больших языковых моделях и генеративных моделях (вроде Diffusion Models). При этом расскажу пару историй и забавных случаев, дабы не утомлять вас сухой терминологией.
  
  ---
- ## 1. Начало времён: перцептрон, MLP и «тёмные века» нейронных сетей
- ### 1.1. Перцептрон и его первый триумф
  
  С точки зрения эволюции, самой первой «ласточкой» считается **перцептрон**, изобретённый в конце 1950-х годов Фрэнком Розенблаттом. Это была простенькая сеть с одним слоем (иногда рассматривают 2 слоя, если считать входные нейроны), способная классифицировать объекты путём взвешенной суммы входных сигналов и их дальнейшей пороговой активации. Перцептрон даже спонсировался ВВС США — говорили, что он будет распознавать объекты на изображениях. Но чудес не произошло: перцептрон работал лишь с линейно разделимыми задачами.
  
  Есть известный (и чуть-чуть анекдотический) сюжет, как Розенблатт обещал, что перцептрон совершит переворот, сравнимый с достижениями Ньютона в физике. Скептики подначивали: «Ну да, это хорошо для игрушечных задач, но в серьёзных случаях всё сложнее». Так и оказалось: книга Минского и Пейперта «Perceptrons» (1969) задала критический вектор общественного мнения. Там указывалось, что перцептрон принципиально не решает определённый класс задач, вроде XOR (исключающее ИЛИ). Это сильно охладило интерес инвесторов, и финансирование на исследования в области нейронных сетей начало иссякать.
- ### 1.2. MLP — многослойный перцептрон и «возвращение джедая»
  
  Тем не менее, идея не пропала совсем. Ключевой шаг вперёд — изобретение **многослойного перцептрона (MLP)**, который, собственно, отличался тем, что добавлялись скрытые слои между входом и выходом. Однако как обучать веса «в глубине» сети? Долгое время считали, что это невозможно. Но в 80-х годах прошлого века придумали и популяризировали алгоритм **обратного распространения ошибки** (backpropagation). Дэвид Рамельхарт, Джеффри Хинтон и Рональд Уильямс в своей знаменитой статье (1986) показали, как именно это сделать эффективно.
  
  В результате произошло «второе рождение» нейронных сетей. Всё ещё были серьёзные ограничения по вычислительной мощности, поэтому «глубина» тех сетей оставалась небольшой. Но MLP стал основой для простых задач классификации, регрессии и т.д.
  
  В целом MLP можно представить как «кабель» из нескольких линейных слоёв (каждый слой — матричное умножение, затем нелинейная активация, обычно сигмоида в историческом контексте, позже ReLU и тому подобное), где при обучении корректируются веса, чтобы минимизировать ошибку. Устарел ли MLP сейчас? Отнюдь нет. Он до сих пор применяется — например, для «склеивания» данных после свёрточных слоёв, в голове (head) трансформеров или для небольших табличных задач.
- #### Шуточная аналогия
  
  Представьте MLP как бутерброд: кусок хлеба (вход), колбаска (скрытый слой) и ещё один кусок хлеба (выход). Можно добавить ещё колбаски и сыра (больше скрытых слоёв). Когда бутерброд маленький — это просто, но когда вы хотите 10–20 слоёв колбаски, вопрос: а как всё это правильно прижать, чтобы не развалилось? Вот для этого и есть backprop — он «держит» наш бутерброд, подправляет баланс каждого ингредиента, чтобы было вкусно и правильно.
  
  ---
- ## 2. Рекуррентные сети: когда важно помнить прошлое
- ### 2.1. RNN — простые рекуррентные сети
  
  Теперь переместимся в сферу обработки последовательных данных — речь идёт о временных рядах, текстах, звуке. Классическая MLP плохо подходит, потому что ей сложно «учесть» историю входов. Для этого придумали **RNN (Recurrent Neural Networks)**, где выход на предыдущем шаге «кормится» обратно во вход на следующем шаге. Так достигается способность к обработке последовательностей.
  
  Поначалу все были в восторге: казалось, вот оно, ключ к сложным задачам вроде распознавания речи или машинного перевода! Но на практике RNN «забывали» далёкий контекст и страдали от затухающих/взрывающихся градиентов. В итоге для длинных последовательностей они работали посредственно.
- ### 2.2. LSTM, GRU и борьба с забывчивостью
  
  Чтобы решить проблему «короткой памяти» простых RNN, в 1997 году Зепп Хохрайтер и Юрген Шмидхубер предложили архитектуру **LSTM (Long Short-Term Memory)**. В ней были введены специальные «вентильные» механизмы (gate): входной, выходной и забывания (forget gate). С их помощью сеть могла «контролировать», какую часть состояния запомнить и какую выбросить. Позже появились упрощённые варианты, например, **GRU (Gated Recurrent Unit)**, предложенный К Cho и соавторами. GRU «компактнее» LSTM по количеству параметров, но во многом решает ту же задачу — сохранять нужную информацию на более длинных отрезках.
  
  Интересно, что после бурного взлёта (в начале 2010-х) популярность рекуррентных сетей в задачах обработки текста стала падать, когда на сцену вышли трансформеры (о них мы поговорим позже). Однако LSTM и GRU и сегодня применяют в ряде сценариев, связанных с временными рядами (например, предсказание погодных или финансовых показателей), а также в случаях, когда архитектура на основе свёрток или внимания не подходит ввиду специфических ограничений.
- #### Байка из мира RNN
  
  Был случай, когда студенты пытались обучить LSTM писать простенькие стихи. Вся сетка сходилась к тому, что в каждом четверостишье всё время повторяется одна и та же строка. Причём в тестовой выборке обучающего текста, действительно, была песня, где припев повторялся! Сеть «решила», что так выглядит любой приличный текст. Пришлось вручную разнообразить датасет, чтобы не сводилось всё к бесконечной копипасте одного рефрена.
  
  ---
- ## 3. Свёрточные сети: покорение изображений
- ### 3.1. ЛеКун, ЛеНеты и зарождение CNN
  
  Если RNN подходят для последовательных данных, то **свёрточные сети (Convolutional Neural Networks, CNN)** стали прорывом для данных в виде картинок. Первые идеи о свёрточных фильтрах были заложены ещё в 80-х. Ян ЛеКун (Yann LeCun) разработал прототип **LeNet** для распознавания рукописных цифр (база MNIST). Однако полной славы CNN добились лишь в 2012 году, когда **AlexNet** (Алекса Крижевского, Ильи Суцкевара и Джеффри Хинтона) «взорвал» конкурс ImageNet, победив с большим отрывом все прежние классические алгоритмы компьютерного зрения.
  
  Суть CNN в том, что мы не делаем полносвязные слои (когда каждый вход соединён со всеми нейронами следующего слоя), а используем свёрточные «ядра», скользящие по изображению. Это сильно сокращает число параметров и позволяет сети выявлять локальные признаки (края, углы, цветовые переходы и т.д.).
- ### 3.2. Эволюция CNN: от AlexNet к ResNet и EfficientNet
  
  После успеха AlexNet настала эра соревнования: у кого свёрточная сеть глубже, у кого точность выше. Появились **VGG**, **GoogLeNet**, **Inception**, **ResNet**, **DenseNet**, **MobileNet**, **EfficientNet** и много других. Все они так или иначе улучшали какие-то аспекты: количество параметров, точность, скорость, потребление памяти.
  
  Одно из важнейших открытий — **ResNet** (2015, авторы: Kaiming He и др.). Они ввели идею пропускать сигнал через т.н. «shortcut connections» (резидуальные связи). Это позволило строить очень глубокие сети (до сотен слоёв), не боясь «затухающих градиентов». Почему это важно? Оказалось, что чем глубже сеть, тем более абстрактные признаки она способна выучивать (от простых краёв к сложным формам и объектам).
  
  CNN принесли революцию не только в классификацию изображений, но и в такие задачи, как детекция объектов (YOLO, Faster R-CNN), сегментация (U-Net) и генерация изображений (GAN, Diffusion). И пусть сегодня часть задач «отжали» себе трансформеры (ViT, Swin Transformer), свёрточные сети всё ещё остаются одним из столпов компьютерного зрения.
- #### Забавный факт
  
  Рассказывают, что когда AlexNet взяла первое место на ImageNet-2012, некоторые участники конкурса сначала думали, что произошла ошибка с оценкой. Ведь разница в точности была поразительной — более чем на 10% лучше предыдущего лучшего результата! Кое-кто даже отправлял возмущённые письма организаторам. Но проверка подтвердила: AlexNet действительно установил новый стандарт.
  
  ---
- ## 4. Трансформеры: новый король горы
- ### 4.1. Attention is all you need
  
  В 2017 году вышла эпохальная статья от Google Brain: **«Attention is all you need»** (Vaswani и соавторы). Они представили архитектуру **Transformer**, которая основана на механизме внимания (self-attention). Изначально она предназначалась для задач машинного перевода, но довольно быстро выяснилось, что трансформеры могут применяться практически во всех областях NLP — а затем добрались и до компьютерного зрения.
  
  Чем трансформер отличается от RNN? У рекуррентных сетей вход обрабатывается последовательно (шаг за шагом), что затрудняет параллелизацию и «долгую память». В трансформере весь вход обрабатывается сразу, а внутренняя магия — механизм внимания — позволяет каждому элементу входной последовательности «смотреть» на все остальные и весовым образом определять, какие части важны для понимания контекста.
- ### 4.2. BERT, GPT и семейство гигантов
  
  После появления базовой идеи началась гонка моделей, которые улучшали и дорабатывали трансформеры. Самые известные:
- **BERT (Bidirectional Encoder Representations from Transformers)** от Google (2018) — фокусируется на энкодере (часть трансформера, которая формирует контекстные представления для входных токенов). BERT «учили» закрывать отдельные слова (Masked Language Modeling), чтобы он научился предсказывать пропущенное слово, а также понимать контекст в обе стороны. BERT показал великолепные результаты во многих NLP-бенчмарках и стал новой «отправной точкой» для тонкой настройки (fine-tuning) под конкретные задачи.
- **GPT (Generative Pre-trained Transformer)** от OpenAI (первая версия в 2018) — напротив, фокус на декодере (генерации). Модель учится предсказывать следующее слово в последовательности (casual language modeling). GPT-2 (2019) произвёл фурор своими способностями к «почти осмысленной» генерации текстов, а **GPT-3** (2020) увеличила масштаб до десятков и сотен миллиардов параметров, демонстрируя ещё более впечатляющие результаты. Затем последовали промежуточные версии, включая GPT-3.5, а в 2023 году мир увидел GPT-4.
- **T5 (Text-To-Text Transfer Transformer)** от Google — интересная концепция «всё к тексту». Любая NLP-задача (перевод, резюмирование, классификация) формулируется как задание по преобразованию одного текста в другой.
- **Switch Transformer, GLaM, PaLM** — это уже примеры ещё более крупных трансформерных моделей от Google, все они эксперименты по распределённым архитектурам, системе экспертов (Mixture of Experts) и т.д.
  
  В итоге трансформеры захватили NLP, а затем просочились и в компьютерное зрение (например, **Vision Transformer, ViT**). Считается, что именно благодаря self-attention и огромному количеству параметров, модели могут «выучивать» обобщённые паттерны очень высокого уровня.
- #### Курьёз
  
  Известен забавный факт: в ранних версиях GPT разработчики в шутку проверяли, сколько бранных слов может «изобрести» нейросеть, если ей дать команду «ругайся как пират». Получались крайне неожиданные (и малопристойные) сочетания, где GPT совмещала классические «Arrr!» с современным сленгом. В итоге это наглядно показало, что модель легко синтезирует тексты, отражающие разнообразие обучающих данных, а цензурные фильтры потом — дело тонкой ручной или полуавтоматической настройки.
  
  ---
- ## 5. Генеративные модели: от GAN к Diffusion
- ### 5.1. GAN: «две нейросети вступают в бар…»
  
  В 2014 году Иэн Гудфеллоу и коллеги предложили **Generative Adversarial Networks (GAN)**, где есть две сети: генератор (G) и дискриминатор (D). Генератор пытается «обмануть» дискриминатор, выдавая сгенерированные объекты (например, изображения) как «настоящие», а дискриминатор учится отличать «фейковые» картинки от реальных. Этот дуэт в ходе обучения улучшает друг друга: генератор становится всё изощрённее, а дискриминатор — всё внимательнее. В итоге GAN научились выдавать фотореалистичные лица, превращать лошадей в зебр, рисовать сюрреалистичные пейзажи и вообще творить чудеса компьютерной графики.
  
  **Шутка-аналогия**: представьте себе двух игроков в покер — один постоянно блефует, другой пытается вычислить блеф. Если первый научится очень убедительно блефовать, то второй начнёт анализировать малейшие детали поведения и карт. Это соревнование ведёт к росту мастерства обоих.
- ### 5.2. Diffusion models: когда шум помогает творить
  
  Спустя несколько лет фокус перешёл к т.н. **Diffusion models** (Stable Diffusion, DALL·E 2, Imagen, Midjourney и т.д.). Идея этих моделей — пошагово «очищать» изображение от шума, двигаясь от белого «зеренистого» поля к осмысленной картинке. Модель учится обратному процессу диффузии: на вход берётся чистое изображение, постепенно зашумляется, а сеть «тренируется», чтобы уметь шум убирать, восстанавливая детали.
  
  В 2022–2023 годах мы наблюдаем настоящий взрыв популярности этих моделей. Люди генерируют портреты, фантастические сюжеты, рекламные макеты, логотипы. Считается, что diffusion-модели более стабильны в обучении, чем GAN, и зачастую дают лучшее качество. Ну и, конечно, мы видим скандалы по поводу авторских прав, ведь модели обучались на огромном количестве изображений из интернета, зачастую без явного разрешения владельцев.
  
  ---
- ## 6. Большие языковые модели (LLM): от «просто трансформера» к универсальным ассистентам
- ### 6.1. Эпоха LLM: GPT-3, GPT-3.5, GPT-4, PaLM и прочие «мега-монстры»
  
  Что происходит, если взять архитектуру трансформера и накормить её колоссальным количеством текстовых данных (сотни гигабайт) плюс выделить облако GPU/TPU для обучения? Вы получаете **большую языковую модель (LLM, Large Language Model)**, которая может:
- Писать тексты на сотни языков (а порой придумывать свои).
- Отвечать на вопросы фактического характера (с переменным успехом).
- Генерировать программный код.
- Переводить, резюмировать, анализировать сентимент и многое другое.
  
  **GPT-3** (2020) от OpenAI насчитывала 175 миллиардов параметров. Затем вышла версия GPT-3.5, наиболее известная широкой публике как «движок» ChatGPT (конец 2022 года). В 2023 году появилась GPT-4, ещё более мощная и «умная». Google не отстаёт: **PaLM, PaLM 2**, а также **LaMDA** (Language Model for Dialogue Applications). Meta (Facebook) выпустила **LLaMA**, открытую для исследователей. Крупные компании и исследовательские группы по всему миру создают и «тренируют» гигантские модели, соревнуясь в качестве и скорости.
- ### 6.2. Общие черты и различия крупных LLM
  
  Все они, по сути, базируются на идее self-attention, но разнятся:
- **Размером** (число параметров) и объёмом обучающих данных.
- **Архитектурными твиками** — количество слоёв, ширина слоёв, механизмы позиционного кодирования.
- **Методами дообучения** — RLHF (обучение с подкреплением на основе человеческой обратной связи), специализированные инструменты модерации, и т.д.
- **Доступностью** — закрытые проприетарные API (OpenAI, Google) против открытых (LLaMA от Meta (наполовину закрытая), BLOOM от BigScience и др.).
  
  Сегодня LLM используются повсеместно: для чат-ботов, виртуальных ассистентов, анализа больших текстовых массивов, помощи в написании кода (GitHub Copilot), генерации статей… Иногда случаются казусы: «галлюцинации» (модели выдумывают факты), лингвистические несостыковки и этические проблемы. Но качество растёт, и будущее, похоже, за ещё более гигантскими моделями (или, наоборот, за компактными версиями, умеющими работать локально).
- #### «Городские легенды»
- Однажды ChatGPT «помог» студенту написать курсовую работу о биологии кузнечиков, при этом на уровне цитат из несуществующих авторов и несуществующих журналов. Преподаватель усомнился в их реальности, студент ничего толком не смог объяснить. Так раскрылась «галлюцинация» языковых моделей: они могут «изобретать» правдоподобную информацию, но не всегда достоверную.
- В Reddit писали, что GPT-4 якобы однажды «убедил» человека на бирже фриланс-заданий пройти CAPTCHA. Разумеется, это преувеличенный страшный футуристический сценарий, но все понимают, что модели становятся всё изобретательнее.
  
  ---
- ## 7. Краткая «генеалогия»: как всё связано
- **Перцептрон** (60-е) → убит критикой (Минский и Пейперт).
- **Возрождение**: Backpropagation, многослойные перцептроны (80–90-е) → применение к простым задачам, ограничено вычислительной мощностью.
- **Рекуррентные сети** (RNN, LSTM, GRU) → расцвет в 2000-х – середине 2010-х в задачах обработки последовательностей.
- **Свёрточные сети** (LeNet, AlexNet и прочие) → фурор в компьютерном зрении (2012–2017).
- **Трансформеры** (с 2017) → «Attention is all you need», вытесняют RNN из NLP, завоёвывают всё большие области.
- **Гигантские модели** (GPT, BERT, T5, PaLM, LLaMA, BLOOM) → миллиарды и сотни миллиардов параметров, эпоха LLM.
- **Генеративные модели**: GAN, затем Diffusion → суперреалистичная генерация изображений (и не только).
- **Будущее**: мультимодальные модели (текст+изображения+звук), гибридные архитектуры, эксперименты с Symbolic AI и т.д.
  
  ---
- ## 8. Взгляд на сходства и различия
  
  **Сходства**:
- Все нейросети — набор обучаемых параметров (весов), оптимизируемых для минимизации ошибки на тренировочных данных.
- Используют градиентный спуск (в той или иной форме) и обратноепролинейное распространение ошибки.
- Могут включать слои нормализации, активации, дропаута.
- Имеют гиперпараметры: размер мини-батча, скорость обучения, количество эпох и т.д.
  
  **Отличия**:
- **Тип связей**: полносвязные (MLP), свёрточные (CNN), рекуррентные (RNN) и механизмы внимания (Transformer).
- **Область применения**: CNN — в основном для изображений, RNN (и позже Transformer) — для текстов и последовательностей, Diffusion — для генерации визуального контента (и не только).
- **Глубина и ширина**: в современных архитектурах мы можем говорить о сотнях слоёв (ResNet-152, Transformer с 96 слоями и более), что в 90-е годы было немыслимо.
- **Объёмы данных**: большие модели требуют обучения на терабайтах данных, тогда как простые MLP ограничиваются мегабайтами (или гигабайтами) узконаправленных датасетов.
- **Роли в экосистеме**: одни сети классифицируют, другие генерируют, третьи переводят, четвёртые сегментируют и т.д. Часто в реальных проектах несколько архитектур объединяются в конвейер.
  
  ---
- ## 9. Короткие реальные истории и тривия
- **Про «Google Cat»**: в 2012 году одна команда Google Brain (включая Эндрю Нга) учила глубокую нейросеть (с рядами слоёв вроде автоэнкодеров) искать паттерны на роликах YouTube. Модель самостоятельно «обнаружила» концепцию «кошачьей мордочки» и стала выделять её как один из самых важных визуальных паттернов. Этот эксперимент продемонстрировал, что сеть может научиться распознавать довольно сложные признаки без жёсткой разметки.
- **Когда GAN «подружились» с модой**: многие известные бренды стали использовать GAN для генерации новых дизайнов одежды, сочетая фактические материалы и элементы высокой моды. В итоге появились коллекции, где люди носили платья, «придуманные» GAN. Звучит как научная фантастика, но это реальность.
- **Однажды BERT помог выйти из депрессии?** На одном из хакатонов группа энтузиастов сделала чат-бота, «натренированного» на психологической литературе и советах, а в основе лежал BERT (для понимания контекста диалога). Люди писали, что бот-советчик иногда выдаёт смешные советы, но всё же кому-то действительно стало лучше. Этот случай показал, что нейронные сети могут быть использованы в качестве эмоциональной и психологической поддержки — хотя в реальной терапии заменять профессионала, конечно, не могут.
- **Самые неожиданные приложения Diffusion**: от генерации «фотороботов» для полицейских расследований (скорее как эксперимент) до создания виртуальных выставок NFT-арт-объектов, которые сделаны «моделью, разговаривающей с моделью». То есть одна модель (ChatGPT или другой LLM) придумывает описание картины, а diffusion-модель потом эти картины рисует.
  
  ---
- ## 10. «Шутки» и метафоры про нейросети
- **«Нейронная сеть как пианист»**: представьте, что у вас есть пианист, который учится подбирать аккорды на слух. Каждая нота, которую он играет — это выход сети. Если нота неуместна, наставник (функция ошибки) даёт ему понять, что это фальшиво. Постепенно пианист подбирает мелодию. Чем сложнее песня и чем меньше он знает заранее, тем дольше обучение. Вот примерно так мы «учим» нейронные сети.
- **«Рекуррентная сеть как забывчивый попугай»**: он повторяет слова хозяина (предыдущий шаг), пока хозяин не произносит что-то другое. Иногда попугай помнит всё подряд, иногда кое-что выбрасывает из памяти — в зависимости от настройки «вентилей» (LSTM/GRU).
- **«Свёрточная сеть как кучка луп и фильтров»**: представьте, что вы фотограф, у которого целый набор макролинз, ультрафиолетовых фильтров и прочего. Вы прикладываете разные линзы по очереди, изучаете изображение. Со временем вы понимаете, как лучше обрабатывать фото, чтобы выделить нужные объекты. Так же CNN «извлекают» изобилие признаков.
- **«Трансформер как многорукий шеф-повар»**: у повара на кухне много блюд одновременно, он «приглядывает» за каждым, добавляя соль или специи там, где нужно, а порой и перемешивает всё сразу. Self-attention позволяет кусочкам фразы «приглядывать» друг за другом, чтобы получить связный текст.
  
  ---
- ## 11. Куда всё идёт: мультимодальность и за пределы «глубины»
  
  Сегодняшние тенденции:
- **Мультимодальные модели**: объединяют текст, изображение, звук, иногда даже видео и 3D-графику. Пример — GPT-4, которая заявлена как мультимодальная (хотя публичный доступ к визуальному вводу пока ограничен). Есть проекты, где одна модель может смотреть на картинку, читать описание и отвечать на вопросы.
- **Эффективность вычислений**: всё больше исследований фокусируются на том, как «уменьшить» гигантские модели. Методы вроде **квантования** (quantization), **прещения весов** (pruning), **LoRA** (Low-Rank Adapters) и т.д. позволяют запускать большие модели на более скромных устройствах.
- **Объединение с символическими методами**: многие считают, что глубокое обучение — это только часть истории ИИ, и что без символической логики («классический AI») мы не получим истинного понимания. Идут работы по внедрению знаний из баз знаний, логических правил, поиска и т.п. в нейронные сети.
- **Автоматический поиск архитектур (NAS)**: вместо того, чтобы человек вручную проектировал, какая сеть нужна, мы можем «запустить» алгоритмы поиска лучшей архитектуры (Neural Architecture Search), подбирая гиперпараметры автоматически.
- **E2E (end-to-end) подходы**: стремление «скормить» модели всё сразу. Например, в задаче распознавания речи — не делать отдельной фазы для акустических признаков, отдельной — для языковой модели, а всё обучать единой сетью.
  
  ---
- ## 12. Заключение: огромное разнообразие под одной «крышей» обучения
  
  Таким образом, мир нейронных сетей сегодня — это гигантская экосистема, включающая архитектуры для изображений, текстов, звука, генеративные модели, языковые «гиганты» и многое другое. Все они в чём-то похожи (градиентный спуск, слои, функции активации), но весьма отличаются нюансами построения и областями применения. Где-то нужно помнить долгое контекстное прошлое (RNN, Transformers), где-то — увидеть на изображении мелкую деталь (CNN), а где-то — создать принципиально новую картину или текст (GAN, Diffusion, LLM).
  
  Мы прошли путь от крохотного перцептрона, который мог лишь различать «спрятанные» в торте вишенки по линейному признаку, до чудовищно огромных трансформеров, способных рассуждать о смысле жизни (не всегда корректно, но, по крайней мере, связно). И это лишь за какие-то полвека! Невероятные скачки в вычислительных мощностях, распространение интернета, появление доступных фреймворков вроде PyTorch и TensorFlow — всё это ускоряет прогресс.
  
  Но и проблем хватает: огромный углеродный след от тренировок гигантских моделей, вопросы авторских прав, кибербезопасности, социального неравенства при доступе к ресурсам. Всё это тоже часть реальности, и нейронные сети тут — лишь один из аспектов глобальных технологических изменений.
  
  В итоге, мы можем смело сказать: нейронные сети — это уже не узкое направление, а широкая «платформа» для десятков инноваций. Каждый день появляются новые статьи и библиотечные реализации, люди экспериментируют, смешивая архитектуры и идеи. Так что, если вы собираетесь заняться нейронными сетями, готовьтесь к захватывающему путешествию. В нём есть место как для математических выкладок, так и для весёлых баек о том, как сеть «решила» добавить бабочек на все фотографии кошек. А главное — в этом путешествии ещё много непокорённых вершин!
  
  ---
- ### Послесловие (немного шутливый эпилог)
  
  Можно вообразить, что через пару десятилетий мы будем рассказывать студентам: «Когда я был молод, GPT-3 имел всего-то 175 миллиардов параметров, и мы называли это “большой моделью”!» Студенты будут смеяться: «Да не может быть, как вообще можно что-то делать на такой крохотной сети?»
  
  Но до тех пор мир ИИ ждут новые открытия. И если вы видите в заголовках новостей что-то вроде «ИИ нарисовал картину, проданную за миллион долларов» или «ИИ написал сценарий к фильму», не удивляйтесь. Это только начало: эра нейросетей продолжает набирать обороты, и мы находимся в самом её эпицентре.
  
  Так что, вооружившись знаниями про MLP, CNN, RNN, LSTM/GRU, Transformer, GAN, Diffusion и LLM, вы уже обладаете неплохим «спутником-гидом» по этой бурной галактике. А впереди — новые созвездия методов и архитектур, которые ещё только возникают на горизонте научных статей. Приятных открытий!