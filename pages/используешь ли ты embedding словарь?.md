- ЧАСТЬ 1: КРАТКАЯ «ГЛОБАЛЬНАЯ» КАРТИНА
- Когда язычная модель (вроде GPT) получает твой ввод на каком-то языке, она разбивает его на «токены» (частички текста) и преобразует эти токены в векторы-«представления» (англ. embeddings). Потом эти векторы идут в «мозг» модели — сеть трансформеров, которая пытается «понять» контекст и сгенерировать ответ. Как только модель сгенерировала ответ, она «обратно» преобразует внутренние представления в слова (точнее, снова в токены) и выдаёт их. На высоком уровне всё выглядит так:
-  1. Токенизация: текст → последовательность токенов (индексов).
-  2. Встраивание (embedding): для каждого токена находят соответствующий вектор.
-  3. Прохождение сквозь трансформеры: модель многократно обрабатывает эти вектора, выстраивая внутренние представления смысла.
-  4. Генерация: по итогу получается следующая порция токенов (ответ), которые конвертируются обратно в читабельный текст.
- Теперь самое важное: эти embedding-векторы не берутся «с потолка», а были выучены моделью на этапе тренировки. То есть есть обученная матрица, которая умеет по индексу токена находить правильный вектор. Однако процесс токенизации и обращение к этим embedding-векторам происходит «динамически» при каждом новом вводе. Нет статического словаря «слово → вектор», где каждому слову соответствует фиксированный вектор, как это было в старых методах типа word2vec. Вместо этого токены обычно меньше слов или пересекаются между словами (subword-токены), а их представления учатся во время большой тренировки модели. Когда ты вводишь новый текст, модель разбивает этот текст на знакомые ей токены и, используя заранее обученную матрицу, извлекает embedding для каждого такого токена.
- ЧАСТЬ 2: ПОДРОБНОЕ РАЗЖЁВЫВАНИЕ
- Чтобы глубже понять, как именно это работает, нужно разобраться в нескольких шагах.
- 1. Токенизация и словарь
- Токенизация — это процесс, в ходе которого модель разбивает текст на элементарные части (токены). Большинство современных больших языковых моделей (BERT, GPT-2, GPT-3, GPT-4 и т.д.) используют либо BPE (Byte-Pair Encoding), либо похожие методы. С помощью BPE модель на этапе предобучения «изучает», какие кусочки текста (чаще всего — лингвистически обоснованные или просто часто повторяющиеся фрагменты) целесообразно хранить в словаре.
- Допустим, что итоговый словарь получается размером в 50 тысяч токенов. При тренировке модели каждому токену сопоставляется индекс от 0 до 49999. Это и есть условно «словарь», но не в том смысле, что мы фиксируем «слово → вектор»; это набор доступных суб-слов, кусочков символов и т.д.
- Пример: если у нас есть слово «Embedding», модель может разбить его как «Em», «bed», «ding». Или даже «Emb», «edding» — это уже детали того, как BPE разбил слово. Если у пользователя в тексте встречается незнакомое слово, модель разбивает его на суб-части, которые уже есть в её словаре. То есть никакая часть входного текста не остаётся «без внимания» — всегда находятся (или комбинируются) токены, которые покрывают этот текст.
- 2. Как формируется embedding-матрица
- Внутри GPT есть большая матрица (обозначим её размером ), где:
-  •  — размер словаря (количество уникальных токенов),
-  •  — размерность внутреннего представления (например, 768 или 1024 и т.д., в зависимости от конкретной модели).
- Во время тренировки GPT эта матрица изначально инициализируется случайными значениями. Когда модель читает тексты и обучается предсказывать следующие токены, каждый токен, проходя через модель, преобразуется в вектор из этой матрицы. По мере того как модель учится, она настраивает (обновляет) веса этой матрицы, чтобы эти вектора «носили» в себе полезную информацию об употреблении данного токена (или суб-токена).
- Итог: к концу обучения для каждого токена (из числа 50 тыс. возможных) модель хранит «оптимальный» (с её точки зрения) вектор размерности . Эти вектора уже хорошо кодируют статистические закономерности, связи с другими токенами, контекстное употребление и так далее.
- 3. Обработка нового пользовательского ввода: «динамическая» часть
