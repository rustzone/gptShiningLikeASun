- при 1 итерации обновления весов мы проходим по всему датасету (на самом деле не по всему датасету а по батчу вроде) (используем все предсказания на каждом шаге) (параллельно?) и корректируем веса так, чтобы модель была универсальной. иначе она перетренится на одном примере и везде будет видеть двойку.
- [[TPU]] — тензорные процессоры
	- очень больших моделей и датасетов вычисления могут быть разделены между несколькими устройствами (несколько GPU или TPU), а иногда и между несколькими машинами (кластером). Каждый узел вычисляет часть градиентов, которые затем объединяются (агрегируются) на центральном узле или по принципу распределённого обновления.